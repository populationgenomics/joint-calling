import collections
import logging
import hail as hl
import pandas as pd

from cpg_utils import to_path
from cpg_utils.config import get_config
from cpg_utils.hail_batch import dataset_path

from hailtop.batch.job import Job

from larcoh.dataproc import add_job
from larcoh.pipeline import Task, Pipeline
from larcoh.utils import exists

logger = logging.getLogger(__file__)


# The target number of rows per partition during each round of merging
TARGET_RECORDS = 25_000

# Default number of partitions in a matrix table generated by combiner
MAX_PARTITIONS = 2586


class Combiner(Task):
    def __init__(self, pipeline: Pipeline):
        raw_combined_mt_path = (
            pipeline.analysis_prefix / f'combiner/{pipeline.output_version}-raw.mt'
        )
        if prev_version := get_config()['workflow'].get('raw_combined_mt_version'):
            prev_analysis_base = to_path(
                dataset_path(f'joint-calling/{prev_version}', category='analysis')
            )
            raw_combined_mt_path = (
                prev_analysis_base / f'combiner/{prev_version}-raw.mt'
            )

        super().__init__(
            pipeline=pipeline,
            name='combiner',
            work_fn=self.work_fn,
            outputs=[raw_combined_mt_path],
        )

    @staticmethod
    def work_fn(self: Task) -> list[Job]:
        logger.info(f'Combining GVCFs')
        assert self.outputs
        out_mt_path = self.outputs[0]

        # Combiner takes advantage of autoscaling cluster policies
        # to reduce costs for the work that uses only the driver machine:
        # https://hail.is/docs/0.2/experimental/vcf_combiner.html#pain-points
        # To add a 50-worker policy for a project "prophecy-339301":
        # ```
        # gcloud dataproc autoscaling-policies import vcf-combiner-50 \
        # --source=combiner-autoscaling-policy-50.yaml --region=australia-southeast1 \
        # --project prophecy-339301
        # ```
        scatter_count = get_config()['workflow'].get('scatter_count', 50)
        if scatter_count > 100:
            autoscaling_workers = '200'
        elif scatter_count > 50:
            autoscaling_workers = '100'
        else:
            autoscaling_workers = '50'

        for i, sample in enumerate(self.pipeline.cohort.get_samples()):
            gvcf_path = sample.get_gvcf_path(access_level='standard')
            if get_config()['workflow'].get('check_inputs', True):
                if not exists(gvcf_path, description=i):
                    if get_config()['workflow'].get(
                        'skip_samples_with_missing_input', False
                    ):
                        logger.warning(
                            f'Skipping {sample} that is missing GVCF {gvcf_path}'
                        )
                        sample.active = False
                    else:
                        raise ValueError(
                            f'Sample {sample} is missing GVCF. '
                            f'Use workflow/skip_samples = [] or '
                            f'workflow/skip_samples_with_missing_input '
                            f'to control behaviour'
                        )

        combiner_tmp_prefix = self.pipeline.tmp_prefix / 'combiner'
        sample_gvcf_tsv_path = combiner_tmp_prefix / 'sample_gvcf.tsv'
        df = pd.DataFrame(
            {'s': sample.id, 'gvcf': sample.get_gvcf_path(access_level='standard')}
            for sample in self.pipeline.cohort.get_samples()
        ).set_index('s', drop=False)
        _check_duplicates(df.s)
        _check_duplicates(df.gvcf)
        with to_path(sample_gvcf_tsv_path).open('w') as f:
            df.to_csv(f, index=False, sep='\t', na_rep='NA')

        logger.info(
            f'Combining {len(self.pipeline.cohort.get_samples())} samples: '
            f'{", ".join(self.pipeline.cohort.get_sample_ids())}'
        )

        branch_factor = get_config().get('combiner', {}).get('branch_factor')
        batch_size = get_config().get('combiner', {}).get('batch_size')
        highmem_workers = get_config().get('dataproc', {}).get('highmem_workers')
        return add_job(
            self.pipeline.b,
            script=(
                f'scripts/combine_gvcfs.py '
                f'--sample-gvcf-tsv {sample_gvcf_tsv_path} '
                f'--out-mt {out_mt_path} '
                f'--tmp-bucket {combiner_tmp_prefix} '
                + (f'--branch-factor {branch_factor} ' if branch_factor else '')
                + (f'--batch-size {batch_size} ' if batch_size else '')
            ),
            job_name=self.name,
            num_workers=0,
            highmem=highmem_workers,
            autoscaling_policy=f'vcf-combiner-{autoscaling_workers}',
            long=True,
        )


def _check_duplicates(items):
    duplicates = [
        item for item, count in collections.Counter(items).items() if count > 1
    ]
    if duplicates:
        raise ValueError(f'Found {len(duplicates)} duplicates: {duplicates}')
    return duplicates
