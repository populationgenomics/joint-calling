import collections
import logging
import pandas as pd

from cpg_utils import to_path, Path
from cpg_utils.config import get_config

from hailtop.batch.job import Job

from larcoh import cohort, batch, dataproc_job, tmp_prefix
from larcoh.dataproc import add_job
from larcoh.utils import exists, can_reuse

logger = logging.getLogger(__file__)


# The target number of rows per partition during each round of merging
TARGET_RECORDS = 25_000

# Default number of partitions in a matrix table generated by combiner
MAX_PARTITIONS = 2586


def queue_jobs(out_mt_path: Path) -> list[Job]:
    """
    Add VCF combiner jobs.
    """
    name = 'VCF Combiner'
    if out_mt_path and not can_reuse(out_mt_path):
        return [batch().new_job(f'{name} [reuse]')]

    # Combiner takes advantage of autoscaling cluster policies
    # to reduce costs for the work that uses only the driver machine:
    # https://hail.is/docs/0.2/experimental/vcf_combiner.html#pain-points
    # To add a 50-worker policy for a project "prophecy-339301":
    # ```
    # gcloud dataproc autoscaling-policies import vcf-combiner-50 \
    # --source=combiner-autoscaling-policy-50.yaml --region=australia-southeast1 \
    # --project prophecy-339301
    # ```
    scatter_count = get_config()['workflow'].get('scatter_count', 50)
    if scatter_count > 100:
        autoscaling_workers = '200'
    elif scatter_count > 50:
        autoscaling_workers = '100'
    else:
        autoscaling_workers = '50'

    for i, sample in enumerate(cohort().get_samples()):
        if not sample.gvcf:
            if get_config()['workflow'].get('skip_samples_with_missing_input', False):
                logger.warning(f'Skipping {sample} which is missing GVCF')
                sample.active = False
                continue
            else:
                raise ValueError(
                    f'Sample {sample} is missing GVCF. '
                    f'Use workflow/skip_samples = [] or '
                    f'workflow/skip_samples_with_missing_input '
                    f'to control behaviour'
                )

        gvcf_path = sample.gvcf.path
        if get_config()['workflow'].get('check_inputs', True):
            if not exists(gvcf_path, description=str(i)):
                if get_config()['workflow'].get(
                    'skip_samples_with_missing_input', False
                ):
                    logger.warning(
                        f'Skipping {sample} that is missing GVCF {gvcf_path}'
                    )
                    sample.active = False
                else:
                    raise ValueError(
                        f'Sample {sample} is missing GVCF. '
                        f'Use workflow/skip_samples = [] or '
                        f'workflow/skip_samples_with_missing_input '
                        f'to control behaviour'
                    )

    logger.info(
        f'Combining {len(cohort().get_samples())} samples: '
        f'{", ".join(cohort().get_sample_ids())}'
    )

    branch_factor = get_config().get('combiner', {}).get('branch_factor')
    batch_size = get_config().get('combiner', {}).get('batch_size')

    job = dataproc_job(
        'combine_gvcfs.py',
        params=dict(
            cohort_tsv=cohort().to_tsv(),
            out_mt=out_mt_path,
            tmp_prefix=tmp_prefix / 'combiner',
            branch_factor=branch_factor,
            batch_size=batch_size,
        ),
        num_workers=0,
        autoscaling_policy=f'vcf-combiner-{autoscaling_workers}',
        long=True,
    )
    return [job]
